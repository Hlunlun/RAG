{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Retrieval Augmented Generation (RAG) from Scratch (step by step tutorial)\n",
    "- [video link ](https://www.youtube.com/watch?v=qN_2fnOPY-M&t=513s)\n",
    "- [source code](https://github.com/mrdbourke/simple-local-rag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements and setup\n",
    "- Check if you have GPU\n",
    "- Environment setupt\n",
    "- Data source (e.g. PDF)\n",
    "- Internet connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import PDF Document\n",
    "download pdf file if we cannot import from local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File data/Jamie Ward - The Student’s Guide to Cognitive Neuroscience-Routledge (2020).pdf exists.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "# directory of data\n",
    "data_dir = 'data'\n",
    "\n",
    "# Get PDF document path\n",
    "pdf_path = 'data/Jamie Ward - The Student’s Guide to Cognitive Neuroscience-Routledge (2020).pdf'\n",
    "\n",
    "if not os.path.exists(pdf_path):\n",
    "    print(\"[INFO] File doesn't exist, downloading...\")\n",
    "    \n",
    "    # Enter the URL of the PDF\n",
    "    url = \"https://download.library.lol/main/3042000/8fa1d36b0def1145a47a1542b8c29e7e/Jamie%20Ward%20-%20The%20Student%E2%80%99s%20Guide%20to%20Cognitive%20Neuroscience-Routledge%20%282020%29.pdf\"\n",
    "\n",
    "\n",
    "    urllib.request.urlretrieve(url, pdf_path)\n",
    "    print('[INFO]File is downloaded')\n",
    "else:\n",
    "    print(f'File {pdf_path} exists.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing PDF File\n",
    "Use [PyMuPDF](https://github.com/pymupdf/pymupdf) to open PDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\CSSA\\anaconda3\\envs\\genai\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "539it [00:01, 472.31it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'page_number': -13,\n",
       "  'page_char_count': 0,\n",
       "  'page_word_count': 1,\n",
       "  'page_setence_count_raw': 1,\n",
       "  'page_token_count': 0.0,\n",
       "  'text': ''},\n",
       " {'page_number': -12,\n",
       "  'page_char_count': 2472,\n",
       "  'page_word_count': 387,\n",
       "  'page_setence_count_raw': 11,\n",
       "  'page_token_count': 618.0,\n",
       "  'text': 'The Student’s Guide to   Cognitive Neuroscience Reflecting recent changes in the way cognition and the brain are studied, this  thoroughly updated fourth edition of this bestselling textbook provides a  comprehensive and student-friendly guide to cognitive neuroscience. Jamie  Ward provides an easy-to-follow introduction to neural structure and function,  as well as all the key methods and procedures of cognitive neuroscience, with  a view to helping students understand how they can be used to shed light on  the neural basis of cognition. The book presents a comprehensive overview of the latest theories and  findings in all the key topics in cognitive neuroscience, including vision,  hearing, attention, memory, speech and language, numeracy, executive  function, social and emotional behavior and developmental neuroscience.  Throughout, case studies, newspaper reports, everyday examples and student- friendly pedagogy are used to help students understand the more challenging  ideas that underpin the subject. New to this edition: •• Increased focus on the impact of genetics on cognition •• New coverage of the cutting-edge field of connectomics •• Coverage of the latest research tools including tES and fNIRS and new  methodologies such as multi-voxel pattern analysis in fMRI research •• Additional content is also included on network versus modular  approaches, brain mechanisms of hand–eye coordination, neurobiological  models of speech perception and production and recent models of  anterior cingulate function. Written in an engaging style by a leading researcher in the field and presented in  full color including numerous illustrative materials, this book will be invaluable  as a core text for undergraduate modules in cognitive neuroscience. It can  also be used as a key text on courses in cognition, cognitive neuropsychology,  biopsychology or brain and behavior. Those embarking on research will find  it an invaluable starting point and reference. This textbook is supported by an extensive companion website for  students and instructors, including lectures by leading researchers, links  to key studies and interviews, interactive multiple-choice questions and  flashcards of key terms. Jamie Ward is Professor of Cognitive Neuroscience at the University of  Sussex, UK. He is the author of a number of books on social and cognitive  neuroscience and on synesthesia, and is President of the British Association  of Cognitive Neuroscience.'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fitz # requires !pip install PyMuPDF, see: https://github.com/pymupdf/PyMuPDF\n",
    "from tqdm.auto import tqdm # pip install tqdm\n",
    "\n",
    "def text_formatter(text: str) -> str:\n",
    "    \"\"\"Performs minor formatting on text.\"\"\"\n",
    "    cleaned_text = text.replace(\"\\n\", \" \").strip() # replace \\n to blank and  remove leading and trailing spaces\n",
    "\n",
    "    # Potentially more text formatting functions can go here\n",
    "    return cleaned_text\n",
    "\n",
    "# This only focues on text, rather than images/figuers etc.\n",
    "def open_and_read_pdf(pdf_path: str) -> list[dict]:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pages_and_texts = []\n",
    "    for page_number, page in tqdm(enumerate(doc)):\n",
    "        text = page.get_text() #get plain text encoded as UTF-8\n",
    "        text = text_formatter(text = text) \n",
    "        pages_and_texts.append({\n",
    "            \"page_number\": page_number - 13,\n",
    "            \"page_char_count\": len(text),\n",
    "            \"page_word_count\": len(text.split(\" \")),\n",
    "            \"page_setence_count_raw\": len(text.split(\". \")),\n",
    "            \"page_token_count\": len(text)/4, # 1 token ~= 4 characters\n",
    "            \"text\": text,\n",
    "        })\n",
    "\n",
    "    return pages_and_texts\n",
    "\n",
    "\n",
    "pages_and_texts = open_and_read_pdf(pdf_path = pdf_path)\n",
    "pages_and_texts[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 69,\n",
       "  'page_char_count': 2234,\n",
       "  'page_word_count': 373,\n",
       "  'page_setence_count_raw': 15,\n",
       "  'page_token_count': 558.5,\n",
       "  'text': '70\\u2003 THE STUDENT’S GUIDE TO COGNITIVE NEUROSCIENCE either had or had not been diagnosed as schizophrenic). Although both groups  showed a number of similar frontal and temporal lobe activities, there was a  strong correlation between activity in these regions in controls and a striking  absence of correlation in the schizophrenics. Friston and Frith (1995) argued  that schizophrenia is best characterized in terms of a failure of communication  between distant brain regions (i.e., a functional disconnection). One commonly used procedure for measuring functional integration  does not use any task at all. These are known as resting state paradigms.  Participants are merely asked to lie back and rest. In the absence of a  task, the fluctuations in brain activity are little more than noise. However,  in brain regions that are functionally connected the noise levels tend to  correlate together. This has enabled researchers to identify sets of networks  in the brain, consisting of spatially separated regions, for which fluctuations  in activity tend to be shared (Damoiseaux et al., 2006). For instance, one  commonly studied network is called the default mode network of the brain  and is implicated in internalized thoughts: it tends to be more active when  not engaged in an experimental task (Raichle et al., 2001). Differences in the  FIGURE 4.13: Every region of the brain shows fluctuations in the BOLD response over time (shown here in detail for two  regions, 1 and 2). If different regions show a very similar profile of fluctuations, then this is likely to reflect the fact that they  are communicating. In resting state paradigms, fluctuations in the BOLD response from all brain regions are entered into a  correlation matrix and, from the pattern of correlations, sets of regions that habitually correlate together (i.e., networks) are  identified. Used with permission from David Essen KEY TERMS Resting state paradigm A technique for  measuring functional  connectivity in which  correlations between  several regions  (networks) are assessed  while the participant is  not performing any tasks. Default mode network A set of brain regions  that is more hemodynam- ically active during rest  than during tasks.'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.sample(pages_and_texts, k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview RAG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_setence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-13</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-12</td>\n",
       "      <td>2472</td>\n",
       "      <td>387</td>\n",
       "      <td>11</td>\n",
       "      <td>618.00</td>\n",
       "      <td>The Student’s Guide to   Cognitive Neuroscienc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-11</td>\n",
       "      <td>647</td>\n",
       "      <td>99</td>\n",
       "      <td>2</td>\n",
       "      <td>161.75</td>\n",
       "      <td>The Student’s Guide to   Cognitive Neuroscienc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-10</td>\n",
       "      <td>75</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>18.75</td>\n",
       "      <td>THE STUDENT’S GUIDE   TO COGNITIVE  ­NEUROSCIE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-9</td>\n",
       "      <td>1461</td>\n",
       "      <td>238</td>\n",
       "      <td>5</td>\n",
       "      <td>365.25</td>\n",
       "      <td>Fourth edition published 2020 by Routledge 2 P...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_number  page_char_count  page_word_count  page_setence_count_raw  \\\n",
       "0          -13                0                1                       1   \n",
       "1          -12             2472              387                      11   \n",
       "2          -11              647               99                       2   \n",
       "3          -10               75               13                       1   \n",
       "4           -9             1461              238                       5   \n",
       "\n",
       "   page_token_count                                               text  \n",
       "0              0.00                                                     \n",
       "1            618.00  The Student’s Guide to   Cognitive Neuroscienc...  \n",
       "2            161.75  The Student’s Guide to   Cognitive Neuroscienc...  \n",
       "3             18.75  THE STUDENT’S GUIDE   TO COGNITIVE  ­NEUROSCIE...  \n",
       "4            365.25  Fourth edition published 2020 by Routledge 2 P...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_setence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>539.00</td>\n",
       "      <td>539.00</td>\n",
       "      <td>539.00</td>\n",
       "      <td>539.00</td>\n",
       "      <td>539.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>256.00</td>\n",
       "      <td>3082.05</td>\n",
       "      <td>514.65</td>\n",
       "      <td>35.69</td>\n",
       "      <td>770.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>155.74</td>\n",
       "      <td>1115.53</td>\n",
       "      <td>181.24</td>\n",
       "      <td>47.41</td>\n",
       "      <td>278.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-13.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>121.50</td>\n",
       "      <td>2382.50</td>\n",
       "      <td>406.00</td>\n",
       "      <td>16.00</td>\n",
       "      <td>595.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>256.00</td>\n",
       "      <td>3121.00</td>\n",
       "      <td>525.00</td>\n",
       "      <td>21.00</td>\n",
       "      <td>780.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>390.50</td>\n",
       "      <td>3709.50</td>\n",
       "      <td>624.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>927.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>525.00</td>\n",
       "      <td>5251.00</td>\n",
       "      <td>871.00</td>\n",
       "      <td>194.00</td>\n",
       "      <td>1312.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  page_char_count  page_word_count  page_setence_count_raw  \\\n",
       "count       539.00           539.00           539.00                  539.00   \n",
       "mean        256.00          3082.05           514.65                   35.69   \n",
       "std         155.74          1115.53           181.24                   47.41   \n",
       "min         -13.00             0.00             1.00                    1.00   \n",
       "25%         121.50          2382.50           406.00                   16.00   \n",
       "50%         256.00          3121.00           525.00                   21.00   \n",
       "75%         390.50          3709.50           624.00                   25.00   \n",
       "max         525.00          5251.00           871.00                  194.00   \n",
       "\n",
       "       page_token_count  \n",
       "count            539.00  \n",
       "mean             770.51  \n",
       "std              278.88  \n",
       "min                0.00  \n",
       "25%              595.62  \n",
       "50%              780.25  \n",
       "75%              927.38  \n",
       "max             1312.75  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token count\n",
    "why would we care about token count?\n",
    "\n",
    "Token count is important to think about because:\n",
    "1. Embedding models don't deal with infinte tokens.\n",
    "2. LLMs don't deal with infinte tokens.\n",
    "\n",
    "For example an embedding model may gave been trained to embed sequences of 384 tokens into numerical space(sentence-transformers `all-mpnet-base-v2`, see: [pretrained_model](https//ww.sbert.net/docs/pretrained_models.html))\n",
    "\n",
    "As for LLMs, they can't accept infinete tokens in their context window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further text processing (splitting pages into sentences)\n",
    "Two ways to do this:\n",
    "1. We've done this by splitting on \". \".\n",
    "2. We can do this with a NLP library such as [spaCy](https://spacy.io/usage) and [nltk](https://www.nltk.org/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Cognitive Neuroscience Reflecting recent changes in the way.]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Add a sentencizer pipeline, see https://spacy.io./api/sentencizer\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "# Create document instance\n",
    "doc = nlp(\"Cognitive Neuroscience Reflecting recent changes in the way.\")\n",
    "assert len(list(doc.sents)) == 1\n",
    "\n",
    "# Print out our sentences split\n",
    "list(doc.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 539/539 [00:02<00:00, 224.46it/s]\n"
     ]
    }
   ],
   "source": [
    "for item in tqdm(pages_and_texts):\n",
    "    item[\"sentences\"] = list(nlp(item[\"text\"]).sents)\n",
    "\n",
    "    # Make sure all sentences are strings (the default type is a spaCy datatype)\n",
    "    item[\"sentences\"] = [str(sentence) for sentence in item[\"sentences\"]]\n",
    "\n",
    "    # Count the senteces\n",
    "    item[\"page_sentence_count_spacy\"] = len(item[\"sentences\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 487,\n",
       "  'page_char_count': 5046,\n",
       "  'page_word_count': 803,\n",
       "  'page_setence_count_raw': 159,\n",
       "  'page_token_count': 1261.5,\n",
       "  'text': '488\\u2003 References Meaney, M. J. (2001). Maternal care, gene expression,  and the transmission of individual differences in  stress reactivity across generations. Annual Review of  Neuroscience, 24, 1161–1192. doi: 10.1146/annurev. neuro.24.1.1161. Mechelli, A., Gorno-Tempini, M. L., & Price, C.  J. (2003). Neuroimaging studies of word and  pseudoword reading: Consistencies, inconsistencies,  and limitations. Journal of Cognitive Neuroscience,  15, 260–271. Mechelli, A., Josephs, O., Ralph, M. A. L., McClelland, J. L.,   & Price, C. J. (2007). Dissociating stimulus-driven  semantic and phonological effect during reading and  naming. Human Brain Mapping, 28(3), 205–217. Medina, J., & Fischer-Baum, S. (2017). Single-case  cognitive neuropsychology in the age of big data.  Cognitive Neuropsychology, 34(7–8), 440–448. doi:  10.1080/02643294.2017.1321537. Mehler, J., Dommergues, J. Y., Frauenfelder, U. H.,  & Segui, J. (1981). The syllable’s role in speech  segmentation. Journal of Verbal Learning and Verbal  Behavior, 20, 298–305. Mekawi, Y., & Bresin, K. (2015). Is the evidence from  racial bias shooting task studies a smoking gun?  Results from a meta-analysis. Journal of Experimental  Social Psychology, 61, 120–130. Meltzoff, A. N., & Moore, M. K. (1977). Imitation of  facial and manual gestures by human neonates.  Science, 198, 75–78. Mendez, M. (2001). Generalized auditory agnosia with  spared music recognition in a left-hander: Analysis  of a case with right temporal stroke. Cortex, 37,  139–150. Merzenich, M. M., Knight, P. L., & Roth, G. L. (1973).  Cochleotopic organization of primary auditory  cortex in the cat. Brain Research, 63, 343–346. Mesulam, M. M. (1999). Spatial attention and neglect:  Parietal, frontal and cingulate contributions to the  mental representation and attentional targeting  of  salient  extrapersonal  events.  Philosophical  Transactions of the Royal Society of London B, 354,  1325–1346. Meuter, R. F. I., & Allport, D. A. (1999). Bilingual  language-switching in naming: Asymmetrical costs  of language selection. Journal of Memory and  Language, 40, 25–40. Mevorach, C., Hodsoll, J., Allen, H., Shalev, L., &  Humphreys, G. (2010). Ignoring the elephant in the  room: A neural circuit to downregulate salience.  Journal of Neuroscience, 30(17), 6072–6079. Mevorach, C., Humphreys, G. W., & Shalev, L. (2006).  Opposite biases in salience-based selection for the  left and right posterior parietal cortex. Nature  Neuroscience, 9(6), 740–742. McCrink, K., Spelke, E. S., Dehaene, S., & Pica, P. (2013).  Non-symbolic halving in an Amazonian indigene  group. Developmental Science, 16(3), 451–462. McDermott, J., & Hauser, M. D. (2007). Nonhuman  primates prefer slow tempos but dislike music overall.  Cognition, 104, 654–668. McGettigan, C., Warren, J. E., Eisner, F., Marshall, C.  R., Shanmugalingam, P., & Scott, S. K. (2011). Neural  correlates of sublexical processing in phonological  working memory. Journal of Cognitive Neuroscience,  23(4), 961–977. McGowan, P. O., Sasaki, A., D’Alessio, A. C., Dymov,  S., Labonte, B., Szyf, M., … Meaney, M. J. (2009).  Epigenetic regulation of the glucocorticoid receptor  in human brain associates with childhood abuse.  Nature Neuroscience, 12(3), 342–348. McGugin, R. W., Gatenby, J. C., Gore, J. C., & Gauthier,  I. (2012). High-resolution imaging of expertise  reveals reliable object selectivity in the fusiform face  area related to perceptual performance. Proceedings  of the National Academy of Sciences of the United  States of America, 109(42), 17063–17068. McGurk, H., & MacDonald, J. (1976). Hearing lips and  seeing voices. Nature, 264, 746–748. McLaughlin, T., & O’Leary, D. D. M. (2005). Molecular  gradients and development of retinotopic maps.  Annual Review of Neuroscience, 28, 327–355. Palo  Alto, CA: Annual Reviews. McLennan, J. E., Nakano, K., Tyler, H. R., & Schwab,  R. S. (1972). Micrographia in Parkinson’s disease.  Journal of Neurological Science, 15, 141–152. McLeod, P., Dittrich, W., Driver, J., Perrett, D., &  Zihl, J. (1996). Preserved and impaired detection of  structure from motion by a “motion-blind” patient.  Visual Cognition, 3, 363–391. McLeod, P., Heywood, C. A., Driver, J., & Zihl, J. (1989).  Selective deficits of visual search in moving displays  after extrastriate damage. Nature, 339, 466–467. McMains, S. A., Fehd, H. M., Emmanouil, T.-A., &  Kastner, S. (2007). Mechanisms of feature- and  space-based attention: Response modulation and  baseline increases. Journal of Neurophysiology, 98(4),  2110–2121. McManus, I. C. (2002). Right hand, left hand. London:  Weidenfeld & Nicolson. McNeil,  J.  E.,  &  Warrington,  E.  K.  (1993).  Prosopagnosia: A face-specific disorder. Quarterly  Journal of Experimental Psychology, 46A, 1–10. McQueen, J. M., & Cutler, A. (2001). Spoken word  access processes: An introduction. Language and  Cognitive Processes, 16, 469–490. Meadows, J. C. (1974). Disturbed perception of colours  associated with localized cerebral lesions. Brain, 97,  615–632.',\n",
       "  'sentences': ['488\\u2003 References Meaney, M. J. (2001).',\n",
       "   'Maternal care, gene expression,  and the transmission of individual differences in  stress reactivity across generations.',\n",
       "   'Annual Review of  Neuroscience, 24, 1161–1192.',\n",
       "   'doi: 10.1146/annurev.',\n",
       "   'neuro.24.1.1161.',\n",
       "   'Mechelli, A., Gorno-Tempini, M. L., & Price, C.  J. (2003).',\n",
       "   'Neuroimaging studies of word and  pseudoword reading: Consistencies, inconsistencies,  and limitations.',\n",
       "   'Journal of Cognitive Neuroscience,  15, 260–271.',\n",
       "   'Mechelli, A., Josephs, O., Ralph, M. A. L., McClelland, J. L.,   & Price, C. J. (2007).',\n",
       "   'Dissociating stimulus-driven  semantic and phonological effect during reading and  naming.',\n",
       "   'Human Brain Mapping, 28(3), 205–217.',\n",
       "   'Medina, J., & Fischer-Baum, S. (2017).',\n",
       "   'Single-case  cognitive neuropsychology in the age of big data.',\n",
       "   ' Cognitive Neuropsychology, 34(7–8), 440–448.',\n",
       "   'doi:  10.1080/02643294.2017.1321537.',\n",
       "   'Mehler, J., Dommergues, J. Y., Frauenfelder, U. H.,  & Segui, J. (1981).',\n",
       "   'The syllable’s role in speech  segmentation.',\n",
       "   'Journal of Verbal Learning and Verbal  Behavior, 20, 298–305.',\n",
       "   'Mekawi, Y., & Bresin, K. (2015).',\n",
       "   'Is the evidence from  racial bias shooting task studies a smoking gun?',\n",
       "   ' Results from a meta-analysis.',\n",
       "   'Journal of Experimental  Social Psychology, 61, 120–130.',\n",
       "   'Meltzoff, A. N., & Moore, M. K. (1977).',\n",
       "   'Imitation of  facial and manual gestures by human neonates.',\n",
       "   ' Science, 198, 75–78.',\n",
       "   'Mendez, M. (2001).',\n",
       "   'Generalized auditory agnosia with  spared music recognition in a left-hander: Analysis  of a case with right temporal stroke.',\n",
       "   'Cortex, 37,  139–150.',\n",
       "   'Merzenich, M. M., Knight, P. L., & Roth, G. L. (1973).',\n",
       "   ' Cochleotopic organization of primary auditory  cortex in the cat.',\n",
       "   'Brain Research, 63, 343–346.',\n",
       "   'Mesulam, M. M. (1999).',\n",
       "   'Spatial attention and neglect:  Parietal, frontal and cingulate contributions to the  mental representation and attentional targeting  of  salient  extrapersonal  events.',\n",
       "   ' Philosophical  Transactions of the Royal Society of London B, 354,  1325–1346.',\n",
       "   'Meuter, R. F. I., & Allport, D. A. (1999).',\n",
       "   'Bilingual  language-switching in naming: Asymmetrical costs  of language selection.',\n",
       "   'Journal of Memory and  Language, 40, 25–40.',\n",
       "   'Mevorach, C., Hodsoll, J., Allen, H., Shalev, L., &  Humphreys, G. (2010).',\n",
       "   'Ignoring the elephant in the  room: A neural circuit to downregulate salience.',\n",
       "   ' Journal of Neuroscience, 30(17), 6072–6079.',\n",
       "   'Mevorach, C., Humphreys, G. W., & Shalev, L. (2006).',\n",
       "   ' Opposite biases in salience-based selection for the  left and right posterior parietal cortex.',\n",
       "   'Nature  Neuroscience, 9(6), 740–742.',\n",
       "   'McCrink, K., Spelke, E. S., Dehaene, S., & Pica, P. (2013).',\n",
       "   ' Non-symbolic halving in an Amazonian indigene  group.',\n",
       "   'Developmental Science, 16(3), 451–462.',\n",
       "   'McDermott, J., & Hauser, M. D. (2007).',\n",
       "   'Nonhuman  primates prefer slow tempos but dislike music overall.',\n",
       "   ' Cognition, 104, 654–668.',\n",
       "   'McGettigan, C., Warren, J. E., Eisner, F., Marshall, C.  R., Shanmugalingam, P., & Scott, S. K. (2011).',\n",
       "   'Neural  correlates of sublexical processing in phonological  working memory.',\n",
       "   'Journal of Cognitive Neuroscience,  23(4), 961–977.',\n",
       "   'McGowan, P. O., Sasaki, A., D’Alessio, A. C., Dymov,  S., Labonte, B., Szyf, M., … Meaney, M. J. (2009).',\n",
       "   ' Epigenetic regulation of the glucocorticoid receptor  in human brain associates with childhood abuse.',\n",
       "   ' Nature Neuroscience, 12(3), 342–348.',\n",
       "   'McGugin, R. W., Gatenby, J. C., Gore, J. C., & Gauthier,  I. (2012).',\n",
       "   'High-resolution imaging of expertise  reveals reliable object selectivity in the fusiform face  area related to perceptual performance.',\n",
       "   'Proceedings  of the National Academy of Sciences of the United  States of America, 109(42), 17063–17068.',\n",
       "   'McGurk, H., & MacDonald, J. (1976).',\n",
       "   'Hearing lips and  seeing voices.',\n",
       "   'Nature, 264, 746–748.',\n",
       "   'McLaughlin, T., & O’Leary, D. D. M. (2005).',\n",
       "   'Molecular  gradients and development of retinotopic maps.',\n",
       "   ' Annual Review of Neuroscience, 28, 327–355.',\n",
       "   'Palo  Alto, CA: Annual Reviews.',\n",
       "   'McLennan, J. E., Nakano, K., Tyler, H. R., & Schwab,  R. S. (1972).',\n",
       "   'Micrographia in Parkinson’s disease.',\n",
       "   ' Journal of Neurological Science, 15, 141–152.',\n",
       "   'McLeod, P., Dittrich, W., Driver, J., Perrett, D., &  Zihl, J. (1996).',\n",
       "   'Preserved and impaired detection of  structure from motion by a “motion-blind” patient.',\n",
       "   ' Visual Cognition, 3, 363–391.',\n",
       "   'McLeod, P., Heywood, C. A., Driver, J., & Zihl, J. (1989).',\n",
       "   ' Selective deficits of visual search in moving displays  after extrastriate damage.',\n",
       "   'Nature, 339, 466–467.',\n",
       "   'McMains, S. A., Fehd, H. M., Emmanouil, T.-A., &  Kastner, S. (2007).',\n",
       "   'Mechanisms of feature- and  space-based attention: Response modulation and  baseline increases.',\n",
       "   'Journal of Neurophysiology, 98(4),  2110–2121.',\n",
       "   'McManus, I. C. (2002).',\n",
       "   'Right hand, left hand.',\n",
       "   'London:  Weidenfeld & Nicolson.',\n",
       "   'McNeil,  J.  E.,  &  Warrington,  E.  K.  (1993).',\n",
       "   ' Prosopagnosia: A face-specific disorder.',\n",
       "   'Quarterly  Journal of Experimental Psychology, 46A, 1–10.',\n",
       "   'McQueen, J. M., & Cutler, A. (2001).',\n",
       "   'Spoken word  access processes: An introduction.',\n",
       "   'Language and  Cognitive Processes, 16, 469–490.',\n",
       "   'Meadows, J. C. (1974).',\n",
       "   'Disturbed perception of colours  associated with localized cerebral lesions.',\n",
       "   'Brain, 97,  615–632.'],\n",
       "  'page_sentence_count_spacy': 89}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(pages_and_texts, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_setence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>page_sentence_count_spacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>539.00</td>\n",
       "      <td>539.00</td>\n",
       "      <td>539.00</td>\n",
       "      <td>539.00</td>\n",
       "      <td>539.00</td>\n",
       "      <td>539.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>256.00</td>\n",
       "      <td>3082.05</td>\n",
       "      <td>514.65</td>\n",
       "      <td>35.69</td>\n",
       "      <td>770.51</td>\n",
       "      <td>29.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>155.74</td>\n",
       "      <td>1115.53</td>\n",
       "      <td>181.24</td>\n",
       "      <td>47.41</td>\n",
       "      <td>278.88</td>\n",
       "      <td>23.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-13.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>121.50</td>\n",
       "      <td>2382.50</td>\n",
       "      <td>406.00</td>\n",
       "      <td>16.00</td>\n",
       "      <td>595.62</td>\n",
       "      <td>18.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>256.00</td>\n",
       "      <td>3121.00</td>\n",
       "      <td>525.00</td>\n",
       "      <td>21.00</td>\n",
       "      <td>780.25</td>\n",
       "      <td>23.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>390.50</td>\n",
       "      <td>3709.50</td>\n",
       "      <td>624.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>927.38</td>\n",
       "      <td>29.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>525.00</td>\n",
       "      <td>5251.00</td>\n",
       "      <td>871.00</td>\n",
       "      <td>194.00</td>\n",
       "      <td>1312.75</td>\n",
       "      <td>102.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  page_char_count  page_word_count  page_setence_count_raw  \\\n",
       "count       539.00           539.00           539.00                  539.00   \n",
       "mean        256.00          3082.05           514.65                   35.69   \n",
       "std         155.74          1115.53           181.24                   47.41   \n",
       "min         -13.00             0.00             1.00                    1.00   \n",
       "25%         121.50          2382.50           406.00                   16.00   \n",
       "50%         256.00          3121.00           525.00                   21.00   \n",
       "75%         390.50          3709.50           624.00                   25.00   \n",
       "max         525.00          5251.00           871.00                  194.00   \n",
       "\n",
       "       page_token_count  page_sentence_count_spacy  \n",
       "count            539.00                     539.00  \n",
       "mean             770.51                      29.35  \n",
       "std              278.88                      23.33  \n",
       "min                0.00                       0.00  \n",
       "25%              595.62                      18.00  \n",
       "50%              780.25                      23.00  \n",
       "75%              927.38                      29.00  \n",
       "max             1312.75                     102.00  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking our sentences together\n",
    "The concept of splitting larger pieces of text into smaller ones is often reffered to as text splitting or chunking.\n",
    "\n",
    "There is 100 % coorrect way to do this.\n",
    "\n",
    "We''ll keep it simple and split into groups of 10 sentences (however, you could alsl try 5, 7, 8, whatever you like).\n",
    "\n",
    "There are frameworkds such as LangChain which can help with this, however, we'll stick with Python for now.\n",
    "https://python.langchain.com/docs/modules/data_connection/document_transformers.\n",
    "\n",
    "\n",
    "Why we do this:\n",
    "1. So our texts are easier to filter (smaller groups of text can be easier to inspect that large passages of text).\n",
    "2. So our text chunk can fit into our embedding model context window (e.g. 384 tokens as a limit).\n",
    "3. So our contexts passed to an LLM can be more specific and focused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 539/539 [00:00<00:00, 180209.63it/s]\n"
     ]
    }
   ],
   "source": [
    "# Define split size to turn groups of sentences into chunks\n",
    "num_sentence_chunk_size = 10\n",
    "\n",
    "# Create a function that recursively splits a list into desired sizes\n",
    "def split_list(input_list: list, slice_size: int) -> list[list[str]]:\n",
    "    \"\"\"\n",
    "    Splits the input_list into sublists of size slice_size (or as close as possible).\n",
    "\n",
    "    For example, a list of 17 sentences would be split into two lists of [[10], [7]]\n",
    "    \"\"\"\n",
    "    return [input_list[i:i + slice_size] for i in range(0, len(input_list), slice_size)]\n",
    "\n",
    "# Loop through pages and texts and split sentences into chunks\n",
    "for item in tqdm(pages_and_texts):\n",
    "    item[\"sentence_chunks\"] = split_list(input_list=item[\"sentences\"], slice_size=num_sentence_chunk_size)\n",
    "    item[\"num_chunks\"] = len(item[\"sentence_chunks\"])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 349,\n",
       "  'page_char_count': 3446,\n",
       "  'page_word_count': 547,\n",
       "  'page_setence_count_raw': 20,\n",
       "  'page_token_count': 861.5,\n",
       "  'text': '350\\u2003 THE STUDENT’S GUIDE TO COGNITIVE NEUROSCIENCE (e.g., Chinese)? The evidence suggests that the same reading system is  indeed used across other languages (Rueckl, et al., 2015), but the different  routes and components may be weighted differently according to the  culture-specific demands. Functional imaging suggests that reading uses similar brain regions across  different languages, albeit to varying degrees. Italian speakers activate more  strongly areas involved in phonemic processing when reading words, whereas  English speakers activate more strongly regions implicated in lexical retrieval  (Paulesu et al., 2000). Studies of Chinese speakers also support a common  network for reading Chinese logographs and reading Roman-alphabetic  transcriptions of Chinese (the latter being a system, called pinyin, used to  help in teaching Chinese; Chen et al., 2002). Reading Chinese logographs  may make more demands on brain regions involved in semantics than reading  English (Chee et al., 2000), and this reveals itself behaviorally in studies  showing that reading logographs is more affected by word imageability than  reading English words (Shibahara et al., 2003). Imageability refers to whether  a concept is concrete or abstract, with concrete words believed to possess  richer semantic representations. Thus, it appears that Chinese readers may  be more reliant on reading via semantics. One possible consequence of this  is that the reading system is more bilateral in the Chinese brain, compared to  reading systems with stronger phonological decoding (Perfetti et al., 2013). Cases of surface dyslexia have been documented in Japanese (Fushimi et al.,  2003) and Chinese (Weekes & Chen, 1999). Reading of Chinese logographs and  Japanese Kanji can be influenced by the parts that comprise them. These parts  have different pronunciations in different contexts, with degree of consistency  varying (Figure 13.15). This is broadly analogous to grapheme–phoneme  regularities in alphabetic scripts. Indeed, the degree of consistency of character– sound correspondence affecting reading of both words and nonwords is  particularly apparent for low-frequency words.  The results suggest that there are nonsemantic  routes for linking print with sound even in scripts  that are not based on the alphabetic principle.  Conversely, phonological dyslexia has been  observed in these scripts, adding further weight  to the notion that the dual-route model may be  universal (Patterson et al., 1996; Yin & Weekes,  2003). Similarly, surface dyslexia (Job et al.,  1983) and phonological dyslexia (De Bastiani  et al., 1988) have been observed in Italian,  even though this reading system is entirely  regular and could, in principle, be achieved by  grapheme–phoneme correspondence alone. As  with English and Chinese, Italian also shows  a word frequency × regularity interaction  for reading aloud in skilled adult readers  (Burani et al., 2006). Less transparent reading systems (e.g.,  English and French) are harder to acquire  than transparent ones (e.g., Italian), and rates  FIGURE 13.15: Although Chinese is not alphabetic, whole  words and characters can nevertheless be decomposed into a  collection of parts. There is evidence to suggest that there is a  separate route that is sensitive to part-based reading of Chinese  characters that is analogous to grapheme–phoneme conversion  in alphabetic scripts. kickimages/iStock',\n",
       "  'sentences': ['350\\u2003 THE STUDENT’S GUIDE TO COGNITIVE NEUROSCIENCE (e.g., Chinese)?',\n",
       "   'The evidence suggests that the same reading system is  indeed used across other languages (Rueckl, et al.,',\n",
       "   '2015), but the different  routes and components may be weighted differently according to the  culture-specific demands.',\n",
       "   'Functional imaging suggests that reading uses similar brain regions across  different languages, albeit to varying degrees.',\n",
       "   'Italian speakers activate more  strongly areas involved in phonemic processing when reading words, whereas  English speakers activate more strongly regions implicated in lexical retrieval  (Paulesu et al.,',\n",
       "   '2000).',\n",
       "   'Studies of Chinese speakers also support a common  network for reading Chinese logographs and reading Roman-alphabetic  transcriptions of Chinese (the latter being a system, called pinyin, used to  help in teaching Chinese; Chen et al.,',\n",
       "   '2002).',\n",
       "   'Reading Chinese logographs  may make more demands on brain regions involved in semantics than reading  English (Chee et al.,',\n",
       "   '2000), and this reveals itself behaviorally in studies  showing that reading logographs is more affected by word imageability than  reading English words (Shibahara et al.,',\n",
       "   '2003).',\n",
       "   'Imageability refers to whether  a concept is concrete or abstract, with concrete words believed to possess  richer semantic representations.',\n",
       "   'Thus, it appears that Chinese readers may  be more reliant on reading via semantics.',\n",
       "   'One possible consequence of this  is that the reading system is more bilateral in the Chinese brain, compared to  reading systems with stronger phonological decoding (Perfetti et al.,',\n",
       "   '2013).',\n",
       "   'Cases of surface dyslexia have been documented in Japanese (Fushimi et al.,',\n",
       "   ' 2003) and Chinese (Weekes & Chen, 1999).',\n",
       "   'Reading of Chinese logographs and  Japanese Kanji can be influenced by the parts that comprise them.',\n",
       "   'These parts  have different pronunciations in different contexts, with degree of consistency  varying (Figure 13.15).',\n",
       "   'This is broadly analogous to grapheme–phoneme  regularities in alphabetic scripts.',\n",
       "   'Indeed, the degree of consistency of character– sound correspondence affecting reading of both words and nonwords is  particularly apparent for low-frequency words.',\n",
       "   ' The results suggest that there are nonsemantic  routes for linking print with sound even in scripts  that are not based on the alphabetic principle.',\n",
       "   ' Conversely, phonological dyslexia has been  observed in these scripts, adding further weight  to the notion that the dual-route model may be  universal (Patterson et al.,',\n",
       "   '1996; Yin & Weekes,  2003).',\n",
       "   'Similarly, surface dyslexia (Job et al.,',\n",
       "   ' 1983) and phonological dyslexia (De Bastiani  et al.,',\n",
       "   '1988) have been observed in Italian,  even though this reading system is entirely  regular and could, in principle, be achieved by  grapheme–phoneme correspondence alone.',\n",
       "   'As  with English and Chinese, Italian also shows  a word frequency × regularity interaction  for reading aloud in skilled adult readers  (Burani et al.,',\n",
       "   '2006).',\n",
       "   'Less transparent reading systems (e.g.,  English and French) are harder to acquire  than transparent ones (e.g., Italian), and rates  FIGURE 13.15: Although Chinese is not alphabetic, whole  words and characters can nevertheless be decomposed into a  collection of parts.',\n",
       "   'There is evidence to suggest that there is a  separate route that is sensitive to part-based reading of Chinese  characters that is analogous to grapheme–phoneme conversion  in alphabetic scripts.',\n",
       "   'kickimages/iStock'],\n",
       "  'page_sentence_count_spacy': 32,\n",
       "  'sentence_chunks': [['350\\u2003 THE STUDENT’S GUIDE TO COGNITIVE NEUROSCIENCE (e.g., Chinese)?',\n",
       "    'The evidence suggests that the same reading system is  indeed used across other languages (Rueckl, et al.,',\n",
       "    '2015), but the different  routes and components may be weighted differently according to the  culture-specific demands.',\n",
       "    'Functional imaging suggests that reading uses similar brain regions across  different languages, albeit to varying degrees.',\n",
       "    'Italian speakers activate more  strongly areas involved in phonemic processing when reading words, whereas  English speakers activate more strongly regions implicated in lexical retrieval  (Paulesu et al.,',\n",
       "    '2000).',\n",
       "    'Studies of Chinese speakers also support a common  network for reading Chinese logographs and reading Roman-alphabetic  transcriptions of Chinese (the latter being a system, called pinyin, used to  help in teaching Chinese; Chen et al.,',\n",
       "    '2002).',\n",
       "    'Reading Chinese logographs  may make more demands on brain regions involved in semantics than reading  English (Chee et al.,',\n",
       "    '2000), and this reveals itself behaviorally in studies  showing that reading logographs is more affected by word imageability than  reading English words (Shibahara et al.,'],\n",
       "   ['2003).',\n",
       "    'Imageability refers to whether  a concept is concrete or abstract, with concrete words believed to possess  richer semantic representations.',\n",
       "    'Thus, it appears that Chinese readers may  be more reliant on reading via semantics.',\n",
       "    'One possible consequence of this  is that the reading system is more bilateral in the Chinese brain, compared to  reading systems with stronger phonological decoding (Perfetti et al.,',\n",
       "    '2013).',\n",
       "    'Cases of surface dyslexia have been documented in Japanese (Fushimi et al.,',\n",
       "    ' 2003) and Chinese (Weekes & Chen, 1999).',\n",
       "    'Reading of Chinese logographs and  Japanese Kanji can be influenced by the parts that comprise them.',\n",
       "    'These parts  have different pronunciations in different contexts, with degree of consistency  varying (Figure 13.15).',\n",
       "    'This is broadly analogous to grapheme–phoneme  regularities in alphabetic scripts.'],\n",
       "   ['Indeed, the degree of consistency of character– sound correspondence affecting reading of both words and nonwords is  particularly apparent for low-frequency words.',\n",
       "    ' The results suggest that there are nonsemantic  routes for linking print with sound even in scripts  that are not based on the alphabetic principle.',\n",
       "    ' Conversely, phonological dyslexia has been  observed in these scripts, adding further weight  to the notion that the dual-route model may be  universal (Patterson et al.,',\n",
       "    '1996; Yin & Weekes,  2003).',\n",
       "    'Similarly, surface dyslexia (Job et al.,',\n",
       "    ' 1983) and phonological dyslexia (De Bastiani  et al.,',\n",
       "    '1988) have been observed in Italian,  even though this reading system is entirely  regular and could, in principle, be achieved by  grapheme–phoneme correspondence alone.',\n",
       "    'As  with English and Chinese, Italian also shows  a word frequency × regularity interaction  for reading aloud in skilled adult readers  (Burani et al.,',\n",
       "    '2006).',\n",
       "    'Less transparent reading systems (e.g.,  English and French) are harder to acquire  than transparent ones (e.g., Italian), and rates  FIGURE 13.15: Although Chinese is not alphabetic, whole  words and characters can nevertheless be decomposed into a  collection of parts.'],\n",
       "   ['There is evidence to suggest that there is a  separate route that is sensitive to part-based reading of Chinese  characters that is analogous to grapheme–phoneme conversion  in alphabetic scripts.',\n",
       "    'kickimages/iStock']],\n",
       "  'num_chunks': 4}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample an example from the group (note: many samples have only 1 chunk as they have <=10 sentences total)\n",
    "random.sample(pages_and_texts, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_setence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>page_sentence_count_spacy</th>\n",
       "      <th>num_chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>539.00</td>\n",
       "      <td>539.00</td>\n",
       "      <td>539.00</td>\n",
       "      <td>539.00</td>\n",
       "      <td>539.00</td>\n",
       "      <td>539.00</td>\n",
       "      <td>539.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>256.00</td>\n",
       "      <td>3082.05</td>\n",
       "      <td>514.65</td>\n",
       "      <td>35.69</td>\n",
       "      <td>770.51</td>\n",
       "      <td>29.35</td>\n",
       "      <td>3.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>155.74</td>\n",
       "      <td>1115.53</td>\n",
       "      <td>181.24</td>\n",
       "      <td>47.41</td>\n",
       "      <td>278.88</td>\n",
       "      <td>23.33</td>\n",
       "      <td>2.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-13.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>121.50</td>\n",
       "      <td>2382.50</td>\n",
       "      <td>406.00</td>\n",
       "      <td>16.00</td>\n",
       "      <td>595.62</td>\n",
       "      <td>18.00</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>256.00</td>\n",
       "      <td>3121.00</td>\n",
       "      <td>525.00</td>\n",
       "      <td>21.00</td>\n",
       "      <td>780.25</td>\n",
       "      <td>23.00</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>390.50</td>\n",
       "      <td>3709.50</td>\n",
       "      <td>624.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>927.38</td>\n",
       "      <td>29.00</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>525.00</td>\n",
       "      <td>5251.00</td>\n",
       "      <td>871.00</td>\n",
       "      <td>194.00</td>\n",
       "      <td>1312.75</td>\n",
       "      <td>102.00</td>\n",
       "      <td>11.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  page_char_count  page_word_count  page_setence_count_raw  \\\n",
       "count       539.00           539.00           539.00                  539.00   \n",
       "mean        256.00          3082.05           514.65                   35.69   \n",
       "std         155.74          1115.53           181.24                   47.41   \n",
       "min         -13.00             0.00             1.00                    1.00   \n",
       "25%         121.50          2382.50           406.00                   16.00   \n",
       "50%         256.00          3121.00           525.00                   21.00   \n",
       "75%         390.50          3709.50           624.00                   25.00   \n",
       "max         525.00          5251.00           871.00                  194.00   \n",
       "\n",
       "       page_token_count  page_sentence_count_spacy  num_chunks  \n",
       "count            539.00                     539.00      539.00  \n",
       "mean             770.51                      29.35        3.38  \n",
       "std              278.88                      23.33        2.35  \n",
       "min                0.00                       0.00        0.00  \n",
       "25%              595.62                      18.00        2.00  \n",
       "50%              780.25                      23.00        3.00  \n",
       "75%              927.38                      29.00        3.00  \n",
       "max             1312.75                     102.00       11.00  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame to get stats\n",
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting each chunk into its own item\n",
    "1. Embed each chunk of sentences into its own numerical representation\n",
    "2. Create new list of dictionaries containing a single chunk of sentences with relative information sucha as page number as well statistics about each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 539/539 [00:00<00:00, 12276.97it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1821"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Split each chunk into its own item\n",
    "pages_and_chunks = []\n",
    "for item in tqdm(pages_and_texts):\n",
    "    for sentence_chunk in item[\"sentence_chunks\"]:\n",
    "        chunk_dict = {}\n",
    "        chunk_dict[\"page_number\"] = item[\"page_number\"]\n",
    "        \n",
    "        # Join the sentences together into a paragraph-like structure, aka a chunk (so they are a single string)\n",
    "        joined_sentence_chunk = \"\".join(sentence_chunk).replace(\"  \", \" \").strip()\n",
    "        # split sentence using \". \" \n",
    "        joined_sentence_chunk = re.sub(r'\\.([A-Z])', r'. \\1', joined_sentence_chunk) # \".A\" -> \". A\" for any full-stop/capital letter combo \n",
    "        chunk_dict[\"sentence_chunk\"] = joined_sentence_chunk\n",
    "\n",
    "        # Get stats about the chunk\n",
    "        chunk_dict[\"chunk_char_count\"] = len(joined_sentence_chunk)\n",
    "        chunk_dict[\"chunk_word_count\"] = len([word for word in joined_sentence_chunk.split(\" \")])\n",
    "        chunk_dict[\"chunk_token_count\"] = len(joined_sentence_chunk) / 4 # 1 token = ~4 characters\n",
    "        \n",
    "        pages_and_chunks.append(chunk_dict)\n",
    "\n",
    "# How many chunks do we have?\n",
    "len(pages_and_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 113,\n",
       "  'sentence_chunk': 'Are these assumptions plausible? • •Critically evaluate the role of group studies in neuropsychological research. • •What are the advantages and disadvantages of using single cases to draw inferences about normal cognitive functioning? • •How have TMS and tDCS studies contributed to our knowledge of brain plasticity? • •Compare and contrast lesion methods arising from organic brain damage with TMS and tES. ONLINE RESOURCES Visit the companion website at www.routledge.com/cw/ward for: • • References to key papers and readings • • Video lectures and interviews on key topics with leading psychologist Elizabeth Warrington and author Jamie Ward, as well as demonstrations of and lectures on brain stimulation • • Multiple-choice questions and interactive flashcards to test your knowledge • • Downloadable glossary',\n",
       "  'chunk_char_count': 817,\n",
       "  'chunk_word_count': 122,\n",
       "  'chunk_token_count': 204.25}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View a random sample\n",
    "random.sample(pages_and_chunks, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>chunk_char_count</th>\n",
       "      <th>chunk_word_count</th>\n",
       "      <th>chunk_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1821.00</td>\n",
       "      <td>1821.00</td>\n",
       "      <td>1821.00</td>\n",
       "      <td>1821.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>310.96</td>\n",
       "      <td>897.43</td>\n",
       "      <td>138.21</td>\n",
       "      <td>224.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>161.79</td>\n",
       "      <td>498.37</td>\n",
       "      <td>76.85</td>\n",
       "      <td>124.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-12.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>169.00</td>\n",
       "      <td>523.00</td>\n",
       "      <td>77.00</td>\n",
       "      <td>130.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>338.00</td>\n",
       "      <td>829.00</td>\n",
       "      <td>130.00</td>\n",
       "      <td>207.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>466.00</td>\n",
       "      <td>1270.00</td>\n",
       "      <td>200.00</td>\n",
       "      <td>317.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>524.00</td>\n",
       "      <td>4439.00</td>\n",
       "      <td>587.00</td>\n",
       "      <td>1109.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  chunk_char_count  chunk_word_count  chunk_token_count\n",
       "count      1821.00           1821.00           1821.00            1821.00\n",
       "mean        310.96            897.43            138.21             224.36\n",
       "std         161.79            498.37             76.85             124.59\n",
       "min         -12.00              5.00              1.00               1.25\n",
       "25%         169.00            523.00             77.00             130.75\n",
       "50%         338.00            829.00            130.00             207.25\n",
       "75%         466.00           1270.00            200.00             317.50\n",
       "max         524.00           4439.00            587.00            1109.75"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(pages_and_chunks)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter chunks of text for short chunks\n",
    "remove item with short sentence in chunk, these chunks may not contain much useful information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk token ocunt: 23.25 | Text: For example, the superior temporal sulcus lies between the superior and medial temporal gyri.\n",
      "Chunk token ocunt: 28.0 | Text: From Barraclough et al. (2005). © 2005 by the Massachusetts Institute of Technology. Reproduced with permission.\n",
      "Chunk token ocunt: 12.0 | Text: Dysgraphia Difficulties in spelling and writing.\n",
      "Chunk token ocunt: 11.25 | Text: A comprehensive selection of advanced topics.\n",
      "Chunk token ocunt: 18.25 | Text: THE STUDENT’S GUIDE  TO COGNITIVE ­NEUROSCIENCE JAMIE WARD Fourth Edition\n"
     ]
    }
   ],
   "source": [
    "# Show random chunks with under 30 tokens in length\n",
    "min_token_length = 30\n",
    "for row in df[df[\"chunk_token_count\"] <= min_token_length].sample(5).iterrows():\n",
    "    print(f'Chunk token ocunt: {row[1][\"chunk_token_count\"]} | Text: {row[1][\"sentence_chunk\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lookds like many of these are headers and footers of different pages.\n",
    "\n",
    "They don't seem to offer too much information.\n",
    "\n",
    "Let's filter our DataFrame/list of dictionaries to only include chunks with over 30 tokens in length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': -12,\n",
       "  'sentence_chunk': 'The Student’s Guide to  Cognitive Neuroscience Reflecting recent changes in the way cognition and the brain are studied, this thoroughly updated fourth edition of this bestselling textbook provides a comprehensive and student-friendly guide to cognitive neuroscience. Jamie Ward provides an easy-to-follow introduction to neural structure and function, as well as all the key methods and procedures of cognitive neuroscience, with a view to helping students understand how they can be used to shed light on the neural basis of cognition. The book presents a comprehensive overview of the latest theories and findings in all the key topics in cognitive neuroscience, including vision, hearing, attention, memory, speech and language, numeracy, executive function, social and emotional behavior and developmental neuroscience. Throughout, case studies, newspaper reports, everyday examples and student- friendly pedagogy are used to help students understand the more challenging ideas that underpin the subject. New to this edition: •• Increased focus on the impact of genetics on cognition •• New coverage of the cutting-edge field of connectomics •• Coverage of the latest research tools including tES and fNIRS and new methodologies such as multi-voxel pattern analysis in fMRI research •• Additional content is also included on network versus modular approaches, brain mechanisms of hand–eye coordination, neurobiological models of speech perception and production and recent models of anterior cingulate function. Written in an engaging style by a leading researcher in the field and presented in full color including numerous illustrative materials, this book will be invaluable as a core text for undergraduate modules in cognitive neuroscience. It can also be used as a key text on courses in cognition, cognitive neuropsychology, biopsychology or brain and behavior. Those embarking on research will find it an invaluable starting point and reference. This textbook is supported by an extensive companion website for students and instructors, including lectures by leading researchers, links to key studies and interviews, interactive multiple-choice questions and flashcards of key terms. Jamie Ward is Professor of Cognitive Neuroscience at the University of Sussex, UK.',\n",
       "  'chunk_char_count': 2279,\n",
       "  'chunk_word_count': 333,\n",
       "  'chunk_token_count': 569.75},\n",
       " {'page_number': -12,\n",
       "  'sentence_chunk': 'He is the author of a number of books on social and cognitive neuroscience and on synesthesia, and is President of the British Association of Cognitive Neuroscience.',\n",
       "  'chunk_char_count': 165,\n",
       "  'chunk_word_count': 27,\n",
       "  'chunk_token_count': 41.25}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages_and_chunks_over_min_token_len = df[df[\"chunk_token_count\"] > min_token_length].to_dict(orient=\"records\")\n",
    "pages_and_chunks_over_min_token_len[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding our text chunks\n",
    "- text -> numbers\n",
    "- similar meaning texts have dimilar numerical representation\n",
    "- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
